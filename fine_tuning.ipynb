{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import preparate_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поменяйте пути к данным на нужные, если они отличаются от текущих"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_path = \"data/source_data/v2_OpenEnded_mscoco_val2014_questions.json\"\n",
    "answer_path = \"data/source_data/v2_mscoco_val2014_annotations.json\"\n",
    "\n",
    "image_source_dir = \"data/source_data/all_images\"\n",
    "data_dir = \"data/preprocessed_data/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preparation...\n",
      "Done creating QA dictionary.\n",
      "Grouping data by image...\n",
      "Full dataset size: 40504\n",
      "Dataset size after trimming: 5\n",
      "Done copying images.\n",
      "Data preparation completed.\n"
     ]
    }
   ],
   "source": [
    "preparate_data(\n",
    "    q_json_path = question_path,\n",
    "    a_json_path = answer_path,\n",
    "    img_src_dir = image_source_dir,\n",
    "    data_dir = data_dir,\n",
    "    data_size = 5 # Увеличьте размер датасета до нужного\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Ju35CtgCkAC",
    "outputId": "6088d3d4-a044-43b1-cee3-9e3d62ea6f43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting deepspeed\n",
      "  Downloading deepspeed-0.14.4.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting peft\n",
      "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting timm\n",
      "  Downloading timm-1.0.7-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mpi4py\n",
      "  Downloading mpi4py-3.1.6.tar.gz (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
      "Collecting hjson (from deepspeed)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ninja (from deepspeed)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-ml-py (from deepspeed)\n",
      "  Downloading nvidia_ml_py-12.555.43-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.7.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.66.4)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.41.2)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.0+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (3.15.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed) (2.18.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Building wheels for collected packages: deepspeed, mpi4py\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for deepspeed: filename=deepspeed-0.14.4-py3-none-any.whl size=1445513 sha256=2ec0821f524ee3baa31d2e5f9ee4014834a9f444f3269d12285d5fa78e4aa27d\n",
      "  Stored in directory: /root/.cache/pip/wheels/8e/bc/a3/608e90bbb301848b78fd75d24d6d43ba3074de968fc0e397ac\n",
      "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for mpi4py: filename=mpi4py-3.1.6-cp310-cp310-linux_x86_64.whl size=2746303 sha256=0a1f70be0716572ec892018da720d54ed2568eaa3f1a1c709fdea9282e0c9090\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/ca/89/8fc1fb1c620afca13bb41c630b1f948bbf446e0aaa4b762e10\n",
      "Successfully built deepspeed mpi4py\n",
      "Installing collected packages: nvidia-ml-py, ninja, hjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mpi4py, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, deepspeed, accelerate, timm, peft\n",
      "Successfully installed accelerate-0.31.0 deepspeed-0.14.4 hjson-3.1.0 mpi4py-3.1.6 ninja-1.11.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py-12.555.43 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 peft-0.11.1 timm-1.0.7\n"
     ]
    }
   ],
   "source": [
    "# установим необходимые библиотеки\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Если используете Google Colab - достаточно лишь этих библиотек\n",
    "# !pip install accelerate deepspeed peft timm mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxNl9tNCccqg"
   },
   "source": [
    "# Fine-Tuning openbmb/MINICMP-V-2\n",
    "Склонируем репозиторий с кодом самой модельки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81WgDd92_JMK",
    "outputId": "d211e8e1-a052-4de6-957d-4579d69c6bcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MiniCPM-V'...\n",
      "remote: Enumerating objects: 1242, done.\u001b[K\n",
      "remote: Counting objects: 100% (701/701), done.\u001b[K\n",
      "remote: Compressing objects: 100% (368/368), done.\u001b[K\n",
      "remote: Total 1242 (delta 520), reused 418 (delta 328), pack-reused 541\u001b[K\n",
      "Receiving objects: 100% (1242/1242), 191.40 MiB | 15.67 MiB/s, done.\n",
      "Resolving deltas: 100% (718/718), done.\n",
      "Updating files: 100% (143/143), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/OpenBMB/MiniCPM-V.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переименуем чтобы не было конфликтов при выполнении команды( на тире в названии ругается)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "e0SdaLwpSzr8"
   },
   "outputs": [],
   "source": [
    "!mv MiniCPM-V MiniCPM_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код ниже нужен лишь в том случае, если используете Google Colab и импортируете данные с вашего Google диска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOLS0nPIZZ5v",
    "outputId": "5d5fc519-3c8f-4eee-a300-bc5552a8695d"
   },
   "outputs": [],
   "source": [
    "# подключим Гугл-диск и возьмём оттуда данные для обучения\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# !cp /content/drive/MyDrive/data.zip .\n",
    "# !unzip data.zip -d ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9WUFkpadYSg"
   },
   "source": [
    "## Код запуска обучения (через командную строку)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Константы и Гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "from scripts import insert_shebang, insert_code\n",
    "\n",
    "GPUS_PER_NODE = 1\n",
    "NNODES = 1\n",
    "NODE_RANK = 0\n",
    "MASTER_ADDR = \"localhost\"\n",
    "MASTER_PORT = 6001\n",
    "\n",
    "MODEL = \"openbmb/MiniCPM-V-2\"\n",
    "LLM_TYPE = \"minicpm\"\n",
    "\n",
    "# Укажите путь к вашим данным\n",
    "TRAIN_DATA_PATH = f\"{data_dir}/train_annot.json\"\n",
    "VALID_DATA_PATH = f\"{data_dir}/valid_annot.json\"\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_STEPS = 110\n",
    "CHECKPOINT_STEPS = 105\n",
    "EVAL_STEPS = 5\n",
    "\n",
    "LR=1e-6\n",
    "W_DECAY = 0.1\n",
    "ADAM_BETA2 = 0.95\n",
    "WARMUP_RATIO = 0.01\n",
    "\n",
    "OPTIMIZER = \"adafactor\"\n",
    "\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "MAX_LENGTH = 1024\n",
    "SLICE_NUMS = 3\n",
    "\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "VALID_BATCH_SIZE = 1\n",
    "\n",
    "# target_modules_regex = \"\\\"lm\\..*layers\\.(?!2|4)\\d+\\.self_attn\\.(q_proj|k_proj)\\\"\"\n",
    "target_modules_regex = \"\\\"llm\\..*layers\\.\\d+\\.self_attn\\.(q_proj|k_proj)\\\"\"\n",
    "\n",
    "# Аргументы для распределённого запуска\n",
    "DISTRIBUTED_ARGS = f\"\"\"\n",
    "    --nproc_per_node {GPUS_PER_NODE} \\\n",
    "    --nnodes {NNODES} \\\n",
    "    --node_rank {NODE_RANK} \\\n",
    "    --master_addr {MASTER_ADDR} \\\n",
    "    --master_port {MASTER_PORT}\n",
    "\"\"\"\n",
    "\n",
    "script_path = \"MiniCPM_V/finetune/finetune.py\"\n",
    "\n",
    "output_dir = \"output/output_minicpmv2_lora\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вставим shebang в начала файла (для того, чтобы ОС понимала, какой интерпретатор нужно использовать).\n",
    "\n",
    "А также отключим распостранение градиента на очень тяжёлый слой в нашей модели(в противном случае, мы не сможем запустить на нашей бесплатной видеокарте T4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_line = \"\\n    \" + \"model.base_model.model.llm.model.embed_tokens.weight.requires_grad = False\"\n",
    "\n",
    "insert_shebang(script_path)\n",
    "insert_code(script_path, code_line, 278)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь запустим обучение, сымитировав вызов через командную строку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BF00DQJkIZ4G"
   },
   "outputs": [],
   "source": [
    "# Делаем файл исполняемым\n",
    "subprocess.run(['chmod', '+x', script_path])\n",
    "\n",
    "command = f\"\"\"\n",
    "torchrun {DISTRIBUTED_ARGS} {script_path} \\\n",
    "    --model_name_or_path {MODEL} \\\n",
    "    --llm_type {LLM_TYPE} \\\n",
    "    --data_path {TRAIN_DATA_PATH} \\\n",
    "    --eval_data_path {VALID_DATA_PATH} \\\n",
    "    --optim {OPTIMIZER} \\\n",
    "    --remove_unused_columns false \\\n",
    "    --label_names \"labels\" \\\n",
    "    --prediction_loss_only false \\\n",
    "    --bf16 true \\\n",
    "    --bf16_full_eval true \\\n",
    "    --fp16 false \\\n",
    "    --fp16_full_eval false \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --tune_vision false \\\n",
    "    --tune_llm false \\\n",
    "    --use_lora true \\\n",
    "    --lora_target_modules {target_modules_regex} \\\n",
    "    --model_max_length {MAX_LENGTH} \\\n",
    "    --max_slice_nums {SLICE_NUMS} \\\n",
    "    --max_steps {MAX_STEPS} \\\n",
    "    --eval_steps {CHECKPOINT_STEPS} \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --logging_dir {output_dir} \\\n",
    "    --logging_strategy \"steps\" \\\n",
    "    --per_device_train_batch_size {TRAIN_BATCH_SIZE} \\\n",
    "    --per_device_eval_batch_size {VALID_BATCH_SIZE} \\\n",
    "    --gradient_accumulation_steps {GRAD_ACCUM_STEPS} \\\n",
    "    --evaluation_strategy \"steps\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps {CHECKPOINT_STEPS} \\\n",
    "    --save_total_limit 10 \\\n",
    "    --learning_rate {LR} \\\n",
    "    --weight_decay {W_DECAY} \\\n",
    "    --warmup_ratio {WARMUP_RATIO} \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --gradient_checkpointing true \\\n",
    "    --deepspeed /content/MiniCPM_V/finetune/ds_config_zero2.json \\\n",
    "    --report_to \"tensorboard\"\n",
    "\"\"\"\n",
    "\n",
    "result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "print(result.stdout)\n",
    "print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTCAVWvOde7R"
   },
   "source": [
    "## Train and Valid Loss\n",
    "Построим графики Loss'a на обучающей и валидационной выборках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import get_latest_checkpoint, plot_loss_from_trainer_state\n",
    "\n",
    "latest_checkpoint = get_latest_checkpoint(output_dir)\n",
    "plot_loss_from_trainer_state(latest_checkpoint + \"/trainer_state.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data(if Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Путь к исходной папке, которую нужно сжать\n",
    "# source_folder = \"output/output_minicpmv2_lora\"\n",
    "\n",
    "# # Путь, куда будет перемещён архив\n",
    "# target_folder = '/content/drive/MyDrive'\n",
    "\n",
    "# current_datetime = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# zip_filename = f'{current_datetime}_final_output'\n",
    "# zip_path = os.path.join(target_folder, zip_filename)\n",
    "\n",
    "# shutil.make_archive(zip_path, 'zip', source_folder)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jup_env",
   "language": "python",
   "name": "jup_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
